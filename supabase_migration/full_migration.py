#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MySQL/MariaDB â†’ Supabase í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
1. MySQLì—ì„œ ëª¨ë“  í…Œì´ë¸” êµ¬ì¡°ì™€ ë°ì´í„°ë¥¼ ë°±ì—…
2. ë°±ì—…ëœ ë°ì´í„°ë¥¼ Supabaseë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
í•œ ë²ˆì— ì‹¤í–‰ ê°€ëŠ¥í•œ í†µí•© ìŠ¤í¬ë¦½íŠ¸
"""

import pymysql
import json
import os
import re
from datetime import datetime
from typing import Dict, List, Any, Optional
import psycopg2
from psycopg2.extras import execute_values
from psycopg2 import sql

# MySQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´
MYSQL_CONFIG = {
    'host': '222.122.198.185',
    'port': 3306,
    'user': 'autofms',
    'password': 'a131150*',
    'db': 'autofms',
    'charset': 'utf8mb4'
}

# Supabase ì„¤ì •
SUPABASE_CONFIG = {
    'project_id': 'yejialakeivdhwntmagf',
    'host': 'aws-1-ap-northeast-2.pooler.supabase.com',  # ì§ì ‘ ì—°ê²° (Dashboardì—ì„œ í™•ì¸ë¨)
    'port': 5432,  # ì§ì ‘ ì—°ê²° í¬íŠ¸ (ë§ˆì´ê·¸ë ˆì´ì…˜ìš©)
    'database': 'postgres',
    'user': 'postgres.yejialakeivdhwntmagf',  # í”„ë¡œì íŠ¸ ID í¬í•¨ ì‚¬ìš©ì ì´ë¦„
    'password': None  # Supabase ë¹„ë°€ë²ˆí˜¸ëŠ” ë³„ë„ë¡œ ì„¤ì • í•„ìš”
}

# ë°±ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
BACKUP_DIR = os.path.join(os.path.dirname(__file__), 'cafe24_backup')
SCHEMA_DIR = os.path.join(BACKUP_DIR, 'schemas')
DATA_DIR = os.path.join(BACKUP_DIR, 'data')

# ë°±ì—…ì—ì„œ ì œì™¸í•  í…Œì´ë¸” ëª©ë¡
EXCLUDED_TABLES = {
    'Board',
    'CHN_batch',
    'CHN_message',
    'Comment',
    'Event_log',
    'FMS_LS',
    'FMS_TS',
    'Junior',
    'Junior_relation',
    'LS_availability',
    'LS_availability_register',
    'LS_confirm',
    'LS_contracts',
    'LS_countings',
    'LS_feedback',
    'LS_history',
    'LS_orders',
    'LS_search_fail',
    'LS_total_history',
    'Locker_bill',
    'Locker_status',
    'Price_table',
    'Priced_FMS',
    'Revisit_discount',
    'Staff',
    'Staff_payment',
    'TS_usage',
    'Term_hold',
    'Term_member',
    'bills',
    'contract_history',
    'contract_history_view',
    'contracts',
    'member_pro_match',
    'members',
    'schedule_adjusted',
    'schedule_weekly_base',
    'staff_pro_mapping',
    'v2_LS_contracts',
    'v2_LS_countings',
    'v2_contract_history',
}


# ==================== ë°±ì—… ê´€ë ¨ í•¨ìˆ˜ ====================

def ensure_directories():
    """ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±"""
    os.makedirs(BACKUP_DIR, exist_ok=True)
    os.makedirs(SCHEMA_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    print(f"ë°±ì—… ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ: {BACKUP_DIR}")


def get_table_list(cursor) -> List[str]:
    """ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  í…Œì´ë¸” ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì œì™¸ í…Œì´ë¸” í•„í„°ë§)"""
    cursor.execute("SHOW TABLES")
    all_tables = [table[0] for table in cursor.fetchall()]
    
    # ì œì™¸í•  í…Œì´ë¸” í•„í„°ë§
    tables = [table for table in all_tables if table not in EXCLUDED_TABLES]
    
    excluded_count = len(all_tables) - len(tables)
    if excluded_count > 0:
        excluded_list = [table for table in all_tables if table in EXCLUDED_TABLES]
        print(f"ì´ {len(all_tables)}ê°œì˜ í…Œì´ë¸” ë°œê²¬")
        print(f"ì œì™¸ëœ í…Œì´ë¸” ({excluded_count}ê°œ): {', '.join(excluded_list)}")
        print(f"ë°±ì—… ëŒ€ìƒ í…Œì´ë¸” ({len(tables)}ê°œ): {', '.join(tables)}")
    else:
        print(f"ì´ {len(tables)}ê°œì˜ í…Œì´ë¸” ë°œê²¬: {', '.join(tables)}")
    
    return tables


def get_table_structure(cursor, table_name: str) -> Dict[str, Any]:
    """í…Œì´ë¸” êµ¬ì¡° ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    # ì»¬ëŸ¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    cursor.execute(f"DESCRIBE `{table_name}`")
    columns = cursor.fetchall()
    
    # ì»¬ëŸ¼ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    column_info = []
    for col in columns:
        column_info.append({
            'Field': col[0],
            'Type': col[1],
            'Null': col[2],
            'Key': col[3],
            'Default': str(col[4]) if col[4] is not None else None,
            'Extra': col[5]
        })
    
    # CREATE TABLE ë¬¸ ê°€ì ¸ì˜¤ê¸°
    cursor.execute(f"SHOW CREATE TABLE `{table_name}`")
    create_table_result = cursor.fetchone()
    create_statement = create_table_result[1] if create_table_result else None
    
    # Check constraint ì •ë³´ ì¶”ì¶œ (CREATE TABLE ë¬¸ì—ì„œ)
    check_constraints = []
    if create_statement:
        # CHECK ì œì•½ ì¡°ê±´ ì°¾ê¸° (ì •ê·œì‹ ì‚¬ìš©)
        check_pattern = r'CHECK\s*\(([^)]+)\)'
        matches = re.finditer(check_pattern, create_statement, re.IGNORECASE)
        for match in matches:
            constraint_expr = match.group(1)
            # ì œì•½ ì¡°ê±´ ì´ë¦„ ì¶”ì¶œ ì‹œë„ (CONSTRAINT name CHECK ...)
            constraint_name_match = re.search(r'CONSTRAINT\s+(\w+)\s+CHECK', create_statement[:match.start()], re.IGNORECASE)
            constraint_name = constraint_name_match.group(1) if constraint_name_match else None
            check_constraints.append({
                'name': constraint_name,
                'expression': constraint_expr
            })
    
    # ì¸ë±ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    cursor.execute(f"SHOW INDEX FROM `{table_name}`")
    indexes = cursor.fetchall()
    
    index_info = []
    for idx in indexes:
        index_info.append({
            'Table': idx[0],
            'Non_unique': idx[1],
            'Key_name': idx[2],
            'Seq_in_index': idx[3],
            'Column_name': idx[4],
            'Collation': idx[5],
            'Cardinality': idx[6],
            'Sub_part': idx[7],
            'Packed': idx[8],
            'Null': idx[9],
            'Index_type': idx[10],
            'Comment': idx[11] if len(idx) > 11 else None
        })
    
    return {
        'table_name': table_name,
        'columns': column_info,
        'create_statement': create_statement,
        'indexes': index_info,
        'check_constraints': check_constraints,
        'backup_timestamp': datetime.now().isoformat()
    }


def save_table_structure(table_name: str, structure: Dict[str, Any]):
    """í…Œì´ë¸” êµ¬ì¡°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    filename = os.path.join(SCHEMA_DIR, f"{table_name}_schema.json")
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(structure, f, ensure_ascii=False, indent=2)
    print(f"  âœ“ êµ¬ì¡° ì €ì¥: {filename}")


def backup_table_data(cursor, table_name: str):
    """í…Œì´ë¸” ë°ì´í„° ë°±ì—…"""
    try:
        # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        cursor.execute(f"SELECT * FROM `{table_name}`")
        columns = [desc[0] for desc in cursor.description]
        rows = cursor.fetchall()
        
        # ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        data = []
        for row in rows:
            row_dict = {}
            for i, col in enumerate(columns):
                value = row[i]
                # datetime, date ë“±ì˜ ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                if isinstance(value, (datetime,)):
                    value = value.isoformat()
                elif hasattr(value, 'isoformat'):
                    value = value.isoformat()
                row_dict[col] = value
            data.append(row_dict)
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        json_filename = os.path.join(DATA_DIR, f"{table_name}_data.json")
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump({
                'table_name': table_name,
                'row_count': len(data),
                'backup_timestamp': datetime.now().isoformat(),
                'data': data
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"  âœ“ ë°ì´í„° ì €ì¥ (JSON): {json_filename} ({len(data)}ê°œ í–‰)")
        
    except Exception as e:
        print(f"  âœ— ë°ì´í„° ë°±ì—… ì‹¤íŒ¨: {str(e)}")


def create_summary_file(tables: List[str], backup_timestamp: str):
    """ë°±ì—… ìš”ì•½ íŒŒì¼ ìƒì„±"""
    summary = {
        'database': MYSQL_CONFIG['db'],
        'host': MYSQL_CONFIG['host'],
        'backup_timestamp': backup_timestamp,
        'total_tables': len(tables),
        'tables': tables,
        'backup_locations': {
            'schemas': SCHEMA_DIR,
            'data_json': DATA_DIR,
        }
    }
    
    summary_filename = os.path.join(BACKUP_DIR, 'backup_summary.json')
    with open(summary_filename, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\në°±ì—… ìš”ì•½ íŒŒì¼ ìƒì„±: {summary_filename}")


def backup_from_mysql():
    """MySQLì—ì„œ ë°±ì—… ìˆ˜í–‰"""
    print("=" * 60)
    print("1ë‹¨ê³„: MySQL ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹œì‘")
    print("=" * 60)
    
    backup_timestamp = datetime.now().isoformat()
    
    # ë””ë ‰í† ë¦¬ ì¤€ë¹„
    ensure_directories()
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    try:
        print(f"\në°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
        db = pymysql.connect(**MYSQL_CONFIG)
        cursor = db.cursor()
        print(f"âœ“ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ: {MYSQL_CONFIG['db']}")
    except Exception as e:
        print(f"âœ— ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return None
    
    try:
        # í…Œì´ë¸” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        print(f"\ní…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ì¤‘...")
        tables = get_table_list(cursor)
        
        if not tables:
            print("ë°±ì—…í•  í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ê° í…Œì´ë¸” ë°±ì—…
        print(f"\ní…Œì´ë¸” ë°±ì—… ì‹œì‘...")
        print("-" * 60)
        
        for i, table_name in enumerate(tables, 1):
            print(f"\n[{i}/{len(tables)}] í…Œì´ë¸”: {table_name}")
            
            try:
                # í…Œì´ë¸” êµ¬ì¡° ë°±ì—…
                structure = get_table_structure(cursor, table_name)
                save_table_structure(table_name, structure)
                
                # í…Œì´ë¸” ë°ì´í„° ë°±ì—…
                backup_table_data(cursor, table_name)
                
            except Exception as e:
                print(f"  âœ— í…Œì´ë¸” ë°±ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                continue
        
        # ë°±ì—… ìš”ì•½ íŒŒì¼ ìƒì„±
        print(f"\n" + "-" * 60)
        create_summary_file(tables, backup_timestamp)
        
        print(f"\n" + "=" * 60)
        print("ë°±ì—… ì™„ë£Œ!")
        print("=" * 60)
        
        return tables
        
    except Exception as e:
        print(f"\nâœ— ë°±ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return None
    
    finally:
        # ì—°ê²° ì¢…ë£Œ
        cursor.close()
        db.close()
        print("\nMySQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")


# ==================== ë§ˆì´ê·¸ë ˆì´ì…˜ ê´€ë ¨ í•¨ìˆ˜ ====================

def load_supabase_password():
    """Supabase ë¹„ë°€ë²ˆí˜¸ ë¡œë“œ (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì„¤ì • íŒŒì¼ì—ì„œ)"""
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¨¼ì € í™•ì¸
    password = os.getenv('SUPABASE_DB_PASSWORD')
    if password:
        return password
    
    # ì„¤ì • íŒŒì¼ì—ì„œ í™•ì¸
    keys_file = os.path.join(os.path.dirname(__file__), 'supabase_keys.json')
    if os.path.exists(keys_file):
        with open(keys_file, 'r', encoding='utf-8') as f:
            keys = json.load(f)
            password = keys.get('db_password')
            if password:
                return password
    
    return None


def mysql_type_to_postgresql(mysql_type: str) -> str:
    """MySQL/MariaDB íƒ€ì…ì„ PostgreSQL íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
    mysql_type = mysql_type.lower().strip()
    
    # int íƒ€ì… ë³€í™˜
    if mysql_type.startswith('int(') or mysql_type == 'int':
        return 'INTEGER'
    elif mysql_type.startswith('bigint(') or mysql_type == 'bigint':
        return 'BIGINT'
    elif mysql_type.startswith('smallint(') or mysql_type == 'smallint':
        return 'SMALLINT'
    elif mysql_type.startswith('tinyint(') or mysql_type == 'tinyint':
        if '1' in mysql_type:
            return 'BOOLEAN'
        return 'SMALLINT'
    
    # varchar, char íƒ€ì…
    if mysql_type.startswith('varchar('):
        match = re.search(r'varchar\((\d+)\)', mysql_type)
        if match:
            size = match.group(1)
            return f'VARCHAR({size})'
        return 'VARCHAR'
    elif mysql_type.startswith('char('):
        match = re.search(r'char\((\d+)\)', mysql_type)
        if match:
            size = match.group(1)
            return f'CHAR({size})'
        return 'CHAR'
    elif mysql_type == 'text':
        return 'TEXT'
    elif mysql_type == 'longtext':
        return 'TEXT'
    elif mysql_type == 'mediumtext':
        return 'TEXT'
    
    # ìˆ«ì íƒ€ì…
    elif mysql_type.startswith('decimal(') or mysql_type.startswith('numeric('):
        return mysql_type.replace('decimal', 'NUMERIC').replace('numeric', 'NUMERIC')
    elif mysql_type.startswith('float(') or mysql_type == 'float':
        return 'REAL'
    elif mysql_type.startswith('double(') or mysql_type == 'double':
        return 'DOUBLE PRECISION'
    
    # ë‚ ì§œ/ì‹œê°„ íƒ€ì…
    elif mysql_type == 'date':
        return 'DATE'
    elif mysql_type == 'time':
        return 'TIME'
    elif mysql_type == 'datetime':
        return 'TIMESTAMP'
    elif mysql_type == 'timestamp':
        return 'TIMESTAMP'
    elif mysql_type.startswith('year(') or mysql_type == 'year':
        return 'INTEGER'
    
    # ê¸°íƒ€
    elif mysql_type == 'blob':
        return 'BYTEA'
    elif mysql_type == 'longblob':
        return 'BYTEA'
    elif mysql_type == 'json':
        return 'JSONB'
    
    return mysql_type.upper()


def convert_default_value(default: Optional[str], pg_type: str) -> Optional[str]:
    """MySQL ê¸°ë³¸ê°’ì„ PostgreSQL ê¸°ë³¸ê°’ìœ¼ë¡œ ë³€í™˜"""
    if default is None:
        return None
    
    default = str(default).strip()
    
    if default.upper() == 'NULL':
        return None
    
    # MySQLì˜ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
    if default in ('0000-00-00 00:00:00', '0000-00-00', '00:00:00'):
        return None
    
    # CURRENT_TIMESTAMP ë³€í™˜ (ê´„í˜¸ ì œê±°)
    if default.upper() in ('CURRENT_TIMESTAMP', 'CURRENT_TIMESTAMP()', 'NOW()', 'NOW'):
        return 'CURRENT_TIMESTAMP'
    
    # PostgreSQLì—ì„œ í•¨ìˆ˜ í˜¸ì¶œì€ ê´„í˜¸ ì—†ì´ ì‚¬ìš©
    if default.upper().endswith('()'):
        func_name = default.upper().replace('()', '')
        if func_name in ('CURRENT_TIMESTAMP', 'NOW', 'CURRENT_DATE', 'CURRENT_TIME'):
            return func_name
    
    # ë¬¸ìì—´ íƒ€ì… ì²˜ë¦¬
    if pg_type.upper().startswith(('VARCHAR', 'CHAR', 'TEXT')):
        if not (default.startswith("'") and default.endswith("'")):
            default = default.replace("'", "''")
            return f"'{default}'"
    
    return default


def generate_postgresql_create_table(schema: Dict[str, Any]) -> tuple:
    """ë°±ì—…ëœ ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ PostgreSQL CREATE TABLE ë¬¸ ìƒì„±"""
    table_name = schema['table_name']
    columns = schema['columns']
    check_constraints = schema.get('check_constraints', [])
    
    pg_table_name = table_name.lower()
    
    column_definitions = []
    primary_keys = []
    
    for col in columns:
        field_name = col['Field'].lower()
        mysql_type = col['Type']
        is_nullable = col['Null'] == 'YES'
        is_primary = col['Key'] == 'PRI'
        default_val = col['Default']
        extra = col.get('Extra', '')
        
        pg_type = mysql_type_to_postgresql(mysql_type)
        
        col_def = f'  {field_name} {pg_type}'
        
        if not is_nullable:
            col_def += ' NOT NULL'
        
        if 'auto_increment' in extra.lower():
            if pg_type == 'INTEGER':
                col_def = col_def.replace('INTEGER', 'SERIAL')
            elif pg_type == 'BIGINT':
                col_def = col_def.replace('BIGINT', 'BIGSERIAL')
        elif default_val is not None:
            pg_default = convert_default_value(default_val, pg_type)
            if pg_default:
                col_def += f' DEFAULT {pg_default}'
        
        column_definitions.append(col_def)
        
        if is_primary:
            primary_keys.append(field_name)
    
    create_sql = f'CREATE TABLE IF NOT EXISTS {pg_table_name} (\n'
    create_sql += ',\n'.join(column_definitions)
    
    if primary_keys:
        create_sql += f',\n  PRIMARY KEY ({", ".join(primary_keys)})\n'
    
    # Check constraint ì²˜ë¦¬
    for constraint in check_constraints:
        constraint_expr = constraint['expression']
        constraint_name = constraint.get('name')
        
        # chat_messages í…Œì´ë¸”ì˜ sender_type check constraint ìˆ˜ì •
        if pg_table_name == 'chat_messages' and 'sender_type' in constraint_expr.lower():
            # sender_type check constraintë¥¼ pro, manager í¬í•¨í•˜ë„ë¡ ìˆ˜ì •
            # MySQL: sender_type IN ('member', 'admin')
            # PostgreSQL: sender_type IN ('member', 'admin', 'pro', 'manager')
            constraint_expr = "sender_type IN ('member', 'admin', 'pro', 'manager')"
            constraint_name = 'chat_messages_sender_type_check'
        
        # PostgreSQL CHECK ì œì•½ ì¡°ê±´ ì¶”ê°€
        if constraint_name:
            create_sql += f',\n  CONSTRAINT {constraint_name} CHECK ({constraint_expr})\n'
        else:
            create_sql += f',\n  CHECK ({constraint_expr})\n'
    
    create_sql += ');'
    
    return create_sql, pg_table_name


def load_table_schema(table_name: str) -> Optional[Dict[str, Any]]:
    """í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ë¡œë“œ"""
    schema_file = os.path.join(SCHEMA_DIR, f"{table_name}_schema.json")
    if not os.path.exists(schema_file):
        return None
    
    with open(schema_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_table_data(table_name: str) -> Optional[List[Dict[str, Any]]]:
    """í…Œì´ë¸” ë°ì´í„° ë¡œë“œ"""
    data_file = os.path.join(DATA_DIR, f"{table_name}_data.json")
    if not os.path.exists(data_file):
        return None
    
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data.get('data', [])


def drop_table_if_exists(cursor, table_name: str):
    """í…Œì´ë¸”ì´ ì¡´ì¬í•˜ë©´ ì‚­ì œ"""
    try:
        cursor.execute(f'DROP TABLE IF EXISTS {table_name} CASCADE;')
        print(f"  âœ“ ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ: {table_name}")
    except Exception as e:
        print(f"  âš  í…Œì´ë¸” ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {str(e)}")


def create_table(cursor, create_sql: str, table_name: str):
    """í…Œì´ë¸” ìƒì„± ë° RLS í™œì„±í™”"""
    try:
        cursor.execute(create_sql)
        print(f"  âœ“ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {table_name}")
        
        # RLS í™œì„±í™” ë° ê¸°ë³¸ ì •ì±… ìƒì„±
        enable_rls_for_table(cursor, table_name)
        
        return True
    except Exception as e:
        print(f"  âœ— í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
        print(f"    SQL: {create_sql[:200]}...")
        return False


def enable_rls_for_table(cursor, table_name: str):
    """í…Œì´ë¸”ì— RLS í™œì„±í™” ë° ê¸°ë³¸ ì •ì±… ìƒì„±"""
    try:
        # 1. RLS í™œì„±í™”
        cursor.execute(f'ALTER TABLE {table_name} ENABLE ROW LEVEL SECURITY')
        print(f"  âœ“ RLS í™œì„±í™”: {table_name}")
        
        # 2. ê¸°ë³¸ ì •ì±… ìƒì„± (ëª¨ë“  ì ‘ê·¼ í—ˆìš© - ê¸°ì¡´ ë™ì‘ ìœ ì§€)
        # ê¸°ì¡´ ì •ì±…ì´ ìˆìœ¼ë©´ ì‚­ì œ
        for policy_suffix in ['allow_all_select', 'allow_all_insert', 'allow_all_update', 'allow_all_delete']:
            full_policy_name = f'{policy_suffix}_{table_name}'
            try:
                cursor.execute(f'DROP POLICY IF EXISTS {full_policy_name} ON {table_name}')
            except:
                pass
        
        # SELECT ì •ì±…
        try:
            cursor.execute(f'''
                CREATE POLICY allow_all_select_{table_name} ON {table_name}
                FOR SELECT
                USING (true)
            ''')
        except Exception as e:
            print(f"  âš  SELECT ì •ì±… ìƒì„± ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {str(e)[:50]}")
        
        # INSERT ì •ì±…
        try:
            cursor.execute(f'''
                CREATE POLICY allow_all_insert_{table_name} ON {table_name}
                FOR INSERT
                WITH CHECK (true)
            ''')
        except Exception as e:
            print(f"  âš  INSERT ì •ì±… ìƒì„± ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {str(e)[:50]}")
        
        # UPDATE ì •ì±…
        try:
            cursor.execute(f'''
                CREATE POLICY allow_all_update_{table_name} ON {table_name}
                FOR UPDATE
                USING (true)
                WITH CHECK (true)
            ''')
        except Exception as e:
            print(f"  âš  UPDATE ì •ì±… ìƒì„± ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {str(e)[:50]}")
        
        # DELETE ì •ì±…
        try:
            cursor.execute(f'''
                CREATE POLICY allow_all_delete_{table_name} ON {table_name}
                FOR DELETE
                USING (true)
            ''')
        except Exception as e:
            print(f"  âš  DELETE ì •ì±… ìƒì„± ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {str(e)[:50]}")
        
        print(f"  âœ“ RLS ì •ì±… ìƒì„± ì™„ë£Œ: {table_name}")
        
    except Exception as e:
        print(f"  âš  RLS í™œì„±í™” ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {str(e)[:50]}")
        # RLS í™œì„±í™” ì‹¤íŒ¨í•´ë„ í…Œì´ë¸” ìƒì„±ì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬


def get_value_case_insensitive(row: Dict[str, Any], col: str) -> Any:
    """ëŒ€ì†Œë¬¸ì ë¬´ê´€í•˜ê²Œ ë”•ì…”ë„ˆë¦¬ì—ì„œ ê°’ ì°¾ê¸°"""
    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í‚¤ ë¨¼ì € ì°¾ê¸°
    if col in row:
        return row[col]
    # ëŒ€ì†Œë¬¸ì ë¬´ê´€í•˜ê²Œ ì°¾ê¸°
    col_lower = col.lower()
    for key in row.keys():
        if key.lower() == col_lower:
            return row[key]
    return None


def insert_table_data(cursor, table_name: str, data: List[Dict[str, Any]]):
    """í…Œì´ë¸” ë°ì´í„° ì‚½ì…"""
    if not data:
        print(f"  âš  ë°ì´í„° ì—†ìŒ: {table_name}")
        return
    
    try:
        # ëª¨ë“  í–‰ì—ì„œ í‚¤ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì™„ì „í•œ í‚¤ ëª©ë¡ ìƒì„± (ëŒ€ì†Œë¬¸ì ë¬´ê´€)
        all_keys = set()
        for row in data:
            for key in row.keys():
                all_keys.add(key.lower())
        
        # ì›ë³¸ í‚¤ì™€ ì†Œë¬¸ì í‚¤ ë§¤í•‘ ìƒì„± (ëª¨ë“  í–‰ì—ì„œ ìˆ˜ì§‘)
        key_mapping = {}
        for row in data:
            for key in row.keys():
                key_lower = key.lower()
                if key_lower not in key_mapping:
                    key_mapping[key_lower] = key
        
        columns = list(all_keys)
        
        # ìŠ¤í‚¤ë§ˆ ë¡œë“œí•˜ì—¬ ì»¬ëŸ¼ íƒ€ì… í™•ì¸
        schema = load_table_schema(table_name)
        column_types = {}
        if schema:
            for col in schema.get('columns', []):
                col_name = col['Field'].lower()
                mysql_type = col['Type'].lower()
                column_types[col_name] = mysql_type
        
        values_list = []
        for row in data:
            values = []
            for col in columns:
                # ëŒ€ì†Œë¬¸ì ë¬´ê´€í•˜ê²Œ ê°’ ê°€ì ¸ì˜¤ê¸°
                value = get_value_case_insensitive(row, col)
                
                if value is None:
                    values.append(None)
                elif isinstance(value, bool):
                    values.append(value)
                elif isinstance(value, (int, float)):
                    values.append(value)
                elif isinstance(value, str):
                    # TIME íƒ€ì…ì— interval ê°’ì´ ë“¤ì–´ê°€ëŠ” ê²½ìš° ì²˜ë¦¬
                    col_type = column_types.get(col, '')
                    if 'time' in col_type and ('interval' in str(value).lower() or 'day' in str(value).lower()):
                        # "1 day, 0:00:00" ê°™ì€ ê°’ì„ TIMEìœ¼ë¡œ ë³€í™˜ ì‹œë„
                        try:
                            # intervalì—ì„œ ì‹œê°„ ë¶€ë¶„ë§Œ ì¶”ì¶œ ì‹œë„
                            if ':' in value:
                                time_part = value.split(',')[-1].strip() if ',' in value else value
                                if ':' in time_part:
                                    values.append(time_part)
                                else:
                                    values.append(None)
                            else:
                                values.append(None)
                        except:
                            values.append(None)
                    # MySQLì˜ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                    elif value in ('0000-00-00 00:00:00', '0000-00-00', '00:00:00'):
                        values.append(None)
                    else:
                        values.append(value)
                else:
                    values.append(str(value))
            
            values_list.append(tuple(values))
        
        table_ident = sql.Identifier(table_name)
        cols_ident = [sql.Identifier(col) for col in columns]
        cols_str = sql.SQL(', ').join(cols_ident)
        
        insert_sql = sql.SQL('INSERT INTO {} ({}) VALUES %s').format(
            table_ident,
            cols_str
        )
        
        batch_size = 1000
        total_inserted = 0
        
        for i in range(0, len(values_list), batch_size):
            batch = values_list[i:i + batch_size]
            execute_values(cursor, insert_sql, batch, page_size=batch_size)
            total_inserted += len(batch)
        
        print(f"  âœ“ ë°ì´í„° ì‚½ì… ì™„ë£Œ: {table_name} ({total_inserted}ê°œ í–‰)")
        
    except Exception as e:
        print(f"  âœ— ë°ì´í„° ì‚½ì… ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()


def migrate_table(cursor, table_name: str):
    """ë‹¨ì¼ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜"""
    schema = load_table_schema(table_name)
    if not schema:
        print(f"  âœ— ìŠ¤í‚¤ë§ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {table_name}")
        return False
    
    create_sql, pg_table_name = generate_postgresql_create_table(schema)
    
    drop_table_if_exists(cursor, pg_table_name)
    
    if not create_table(cursor, create_sql, pg_table_name):
        return False
    
    data = load_table_data(table_name)
    if data:
        insert_table_data(cursor, pg_table_name, data)
    else:
        print(f"  âš  ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {table_name}")
    
    return True


def parse_connection_string(conn_str: str) -> dict:
    """Supabase ì—°ê²° ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ì„¤ì • ì¶”ì¶œ"""
    import urllib.parse
    # postgresql://postgres:password@host:port/database í˜•ì‹ íŒŒì‹±
    parsed = urllib.parse.urlparse(conn_str)
    return {
        'host': parsed.hostname,
        'port': parsed.port or 5432,
        'database': parsed.path.lstrip('/') if parsed.path else 'postgres',
        'user': parsed.username,
        'password': parsed.password
    }


def migrate_to_supabase(tables: List[str]):
    """Supabaseë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ìˆ˜í–‰"""
    print("\n" + "=" * 60)
    print("2ë‹¨ê³„: Supabase ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
    print("=" * 60)
    
    project_url = f"https://supabase.com/dashboard/project/{SUPABASE_CONFIG['project_id']}"
    print(f"\nğŸ“‹ Supabase í”„ë¡œì íŠ¸ ì •ë³´:")
    print(f"   í”„ë¡œì íŠ¸ ID: {SUPABASE_CONFIG['project_id']}")
    print(f"   í”„ë¡œì íŠ¸ URL: {project_url}")
    
    # ì—°ê²° ë¬¸ìì—´ì´ ì„¤ì • íŒŒì¼ì— ìˆëŠ”ì§€ í™•ì¸
    keys_file = os.path.join(os.path.dirname(__file__), 'supabase_keys.json')
    connection_string = None
    if os.path.exists(keys_file):
        with open(keys_file, 'r', encoding='utf-8') as f:
            keys = json.load(f)
            connection_string = keys.get('connection_string')
    
    # ì—°ê²° ë¬¸ìì—´ì´ ìˆìœ¼ë©´ íŒŒì‹±í•˜ì—¬ ì‚¬ìš©
    if connection_string:
        print(f"\nâœ“ ì—°ê²° ë¬¸ìì—´ ë°œê²¬, íŒŒì‹± ì¤‘...")
        try:
            parsed = parse_connection_string(connection_string)
            # ë¹„ë°€ë²ˆí˜¸ëŠ” ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œí•œ ê²ƒì„ ì‚¬ìš© (ì—°ê²° ë¬¸ìì—´ì˜ ë¹„ë°€ë²ˆí˜¸ ë¬´ì‹œ)
            password_backup = SUPABASE_CONFIG.get('password')
            SUPABASE_CONFIG.update(parsed)
            # ë¹„ë°€ë²ˆí˜¸ëŠ” ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œí•œ ê²ƒì„ ì‚¬ìš©
            if password_backup:
                SUPABASE_CONFIG['password'] = password_backup
            print(f"   í˜¸ìŠ¤íŠ¸: {SUPABASE_CONFIG['host']}")
            print(f"   í¬íŠ¸: {SUPABASE_CONFIG['port']}")
            print(f"   ì‚¬ìš©ì: {SUPABASE_CONFIG['user']}")
            print(f"   ë°ì´í„°ë² ì´ìŠ¤: {SUPABASE_CONFIG['database']}")
        except Exception as e:
            print(f"   âš  ì—°ê²° ë¬¸ìì—´ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            print(f"   ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
    else:
        print(f"\nğŸ“‹ í˜„ì¬ ì—°ê²° ì„¤ì •:")
        print(f"   í˜¸ìŠ¤íŠ¸: {SUPABASE_CONFIG['host']}")
        print(f"   í¬íŠ¸: {SUPABASE_CONFIG['port']}")
        print(f"   ì‚¬ìš©ì: {SUPABASE_CONFIG['user']}")
        print(f"   ë°ì´í„°ë² ì´ìŠ¤: {SUPABASE_CONFIG['database']}")
    
    # Supabase ë¹„ë°€ë²ˆí˜¸ í™•ì¸
    password = load_supabase_password()
    if not password:
        print("\nâœ— Supabase ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   supabase_keys.json íŒŒì¼ì— 'db_password' í‚¤ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜")
        print("   í™˜ê²½ ë³€ìˆ˜ SUPABASE_DB_PASSWORDë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return False
    
    SUPABASE_CONFIG['password'] = password
    print(f"âœ“ ë¹„ë°€ë²ˆí˜¸ í™•ì¸ ì™„ë£Œ (ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œë¨)")
    
    # Supabase ì—°ê²°
    try:
        print(f"\nSupabase ì§ì ‘ ì—°ê²° ì‹œë„ ì¤‘...")
        print(f"í˜¸ìŠ¤íŠ¸: {SUPABASE_CONFIG['host']}")
        print(f"í¬íŠ¸: {SUPABASE_CONFIG['port']}")
        print(f"ë°ì´í„°ë² ì´ìŠ¤: {SUPABASE_CONFIG['database']}")
        print(f"ì‚¬ìš©ì: {SUPABASE_CONFIG['user']}")
        
        # ì—°ê²° ë¬¸ìì—´ì—ì„œ íŒŒì‹±í•œ ì •ë³´ ì‚¬ìš© (ì§ì ‘ ì—°ê²°)
        conn_params = {
            'host': SUPABASE_CONFIG['host'],
            'port': SUPABASE_CONFIG['port'],
            'database': SUPABASE_CONFIG['database'],
            'user': SUPABASE_CONFIG['user'],
            'password': SUPABASE_CONFIG['password'],
            'sslmode': 'require',
            'connect_timeout': 10
        }
        
        conn = psycopg2.connect(**conn_params)
        conn.autocommit = False
        cursor = conn.cursor()
        print(f"âœ“ Supabase ì—°ê²° ì„±ê³µ!")
        
    except Exception as e:
        print(f"âœ— ì§ì ‘ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        print(f"\ní’€ëŸ¬ ì—°ê²°ë¡œ ì¬ì‹œë„ ì¤‘...")
        
        # í’€ëŸ¬ ì—°ê²°ë¡œ ì¬ì‹œë„
        try:
            # í’€ëŸ¬ ì—°ê²° ë¬¸ìì—´ í™•ì¸
            keys_file = os.path.join(os.path.dirname(__file__), 'supabase_keys.json')
            pooler_connection_string = None
            if os.path.exists(keys_file):
                with open(keys_file, 'r', encoding='utf-8') as f:
                    keys = json.load(f)
                    pooler_connection_string = keys.get('pooler_connection_string')
            
            if pooler_connection_string:
                # í’€ëŸ¬ ì—°ê²° ë¬¸ìì—´ íŒŒì‹±
                parsed = parse_connection_string(pooler_connection_string)
                pooler_params = {
                    'host': parsed['host'],
                    'port': parsed['port'],
                    'database': parsed['database'],
                    'user': parsed['user'],
                    'password': SUPABASE_CONFIG['password'],
                    'sslmode': 'require',
                    'connect_timeout': 10
                }
            else:
                # ê¸°ë³¸ í’€ëŸ¬ ì„¤ì •
                pooler_params = {
                    'host': 'aws-1-ap-northeast-2.pooler.supabase.com',
                    'port': 6543,
                    'database': 'postgres',
                    'user': 'postgres.yejialakeivdhwntmagf',
                    'password': SUPABASE_CONFIG['password'],
                    'sslmode': 'require',
                    'connect_timeout': 10
                }
            
            print(f"  í˜¸ìŠ¤íŠ¸: {pooler_params['host']}")
            print(f"  í¬íŠ¸: {pooler_params['port']}")
            print(f"  ì‚¬ìš©ì: {pooler_params['user']}")
            
            conn = psycopg2.connect(**pooler_params)
            conn.autocommit = False
            cursor = conn.cursor()
            print(f"âœ“ Supabase ì—°ê²° ì„±ê³µ (í’€ëŸ¬ ì—°ê²°)")
            
        except Exception as e2:
            print(f"âœ— í’€ëŸ¬ ì—°ê²°ë„ ì‹¤íŒ¨: {str(e2)}")
            print(f"\nâœ— ëª¨ë“  ì—°ê²° ì‹œë„ ì‹¤íŒ¨")
            print(f"\nì—°ê²° ì •ë³´ í™•ì¸:")
            print(f"  í”„ë¡œì íŠ¸ ID: {SUPABASE_CONFIG['project_id']}")
            print(f"  Supabase Dashboardì—ì„œ ì—°ê²° ë¬¸ìì—´ì„ í™•ì¸í•˜ì„¸ìš”:")
            print(f"  https://supabase.com/dashboard/project/{SUPABASE_CONFIG['project_id']}/settings/database")
            print(f"\nì—°ê²° ë¬¸ìì—´ì„ supabase_keys.jsonì˜ 'connection_string'ì— ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš”.")
            import traceback
            traceback.print_exc()
            return False
    
    # ì§ì ‘ ì—°ê²° ì„±ê³µ ì‹œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
    try:
        print(f"\ní…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘ (ì§ì ‘ ì—°ê²°)...")
        print("-" * 60)
        
        success_count = 0
        fail_count = 0
        
        for i, table_name in enumerate(tables, 1):
            print(f"\n[{i}/{len(tables)}] {table_name}")
            
            try:
                if migrate_table(cursor, table_name):
                    conn.commit()
                    success_count += 1
                else:
                    conn.rollback()
                    fail_count += 1
            except Exception as e:
                print(f"  âœ— ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                conn.rollback()
                fail_count += 1
                import traceback
                traceback.print_exc()
                continue
        
        print(f"\n" + "=" * 60)
        print("ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
        print(f"ì„±ê³µ: {success_count}ê°œ í…Œì´ë¸”")
        print(f"ì‹¤íŒ¨: {fail_count}ê°œ í…Œì´ë¸”")
        print("=" * 60)
        
        # ì‹œí€€ìŠ¤ ì¬ì„¤ì •
        print(f"\n" + "=" * 60)
        print("3ë‹¨ê³„: ì‹œí€€ìŠ¤ ì¬ì„¤ì •")
        print("=" * 60)
        reset_all_sequences(cursor)
        conn.commit()
        
        return True
        
    except Exception as e:
        print(f"\nâœ— ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        conn.rollback()
        return False
    
    finally:
        cursor.close()
        conn.close()
        print("\nSupabase ì—°ê²° ì¢…ë£Œ")


# ==================== ì‹œí€€ìŠ¤ ì¬ì„¤ì • í•¨ìˆ˜ ====================

# SERIAL(ìë™ ì¦ê°€) ì»¬ëŸ¼ì´ ìˆëŠ” í…Œì´ë¸” ëª©ë¡
# supabase_adapter.dartì˜ _tableAutoIncrementColumnsì™€ ë™ê¸°í™” í•„ìš”
SERIAL_COLUMNS = {
    # v3 í…Œì´ë¸”
    'v3_contract_history': 'contract_history_id',
    'v3_ls_countings': 'ls_counting_id',
    'v3_members': 'member_id',
    # v2 ê²°ì œ/ì²­êµ¬ ê´€ë ¨
    'v2_bills': 'bill_id',
    'v2_bill_term': 'bill_term_id',
    'v2_bill_term_hold': 'term_hold_id',
    'v2_bill_times': 'bill_min_id',
    'v2_bill_games': 'bill_game_id',
    'v2_bill_games_group': 'group_play_id',
    # v2 íšŒì›/ê³„ì•½ ê´€ë ¨
    'v2_members': 'member_id',
    'v2_contracts': 'contract_id',
    'v2_member_pro_match': 'member_pro_relation_id',
    # v2 ê²Œì‹œíŒ ê´€ë ¨
    'v2_board': 'board_id',
    'v2_board_by_member': 'memberboard_id',
    'v2_board_by_member_replies': 'reply_id',
    'v2_board_comment': 'comment_id',
    # v2 ë½ì»¤ ê´€ë ¨
    'v2_locker_status': 'locker_id',
    'v2_locker_bill': 'locker_bill_id',
    # v2 ë©”ì‹œì§€/ê²°ì œ
    'v2_message': 'msg_id',
    'v2_portone_payments': 'portone_payment_id',
    # v2 ìŠ¤ì¼€ì¤„/ì§ì› ê´€ë ¨
    'v2_schedule_adjusted_pro': 'scheduled_staff_id',
    'v2_schedule_adjusted_manager': 'scheduled_staff_id',
    'v2_staff_pro': 'pro_contract_id',
    'v2_staff_manager': 'manager_contract_id',
    # v2 ê¸°íƒ€
    'v2_term_member': 'term_id',
    'v2_discount_coupon': 'coupon_id',
    'v2_discount_coupon_auto_triggers': 'trigger_id',
    'v2_ls_orders': 'ls_order_id',
    'v2_wol_settings': 'pc_id',
}


def reset_all_sequences(cursor):
    """ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ ëª¨ë“  SERIAL ì»¬ëŸ¼ì˜ ì‹œí€€ìŠ¤ë¥¼ ì¬ì„¤ì •"""
    print("\nì‹œí€€ìŠ¤ ì¬ì„¤ì • ì‹œì‘...")
    print("-" * 60)
    
    success_count = 0
    fail_count = 0
    
    for table_name, column_name in SERIAL_COLUMNS.items():
        try:
            # í•´ë‹¹ í…Œì´ë¸”ì˜ ìµœëŒ€ ID ì¡°íšŒ
            cursor.execute(f"SELECT MAX({column_name}) FROM {table_name}")
            result = cursor.fetchone()
            max_id = result[0] if result[0] is not None else 0
            
            # ì‹œí€€ìŠ¤ ì¬ì„¤ì •
            cursor.execute(f"""
                SELECT setval(
                    pg_get_serial_sequence('{table_name}', '{column_name}'), 
                    {max_id}, 
                    true
                )
            """)
            
            print(f"  âœ“ {table_name}.{column_name}: ì‹œí€€ìŠ¤ë¥¼ {max_id}ë¡œ ì¬ì„¤ì •")
            success_count += 1
            
        except Exception as e:
            # í…Œì´ë¸”ì´ë‚˜ ì‹œí€€ìŠ¤ê°€ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
            print(f"  âš  {table_name}.{column_name}: ê±´ë„ˆëœ€ ({str(e)[:50]}...)")
            fail_count += 1
            continue
    
    print("-" * 60)
    print(f"ì‹œí€€ìŠ¤ ì¬ì„¤ì • ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ê±´ë„ˆëœ€ {fail_count}ê°œ")


# ==================== ë©”ì¸ í•¨ìˆ˜ ====================

def migrate_single_table(table_name: str):
    """ë‹¨ì¼ í…Œì´ë¸”ë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜ (ë°±ì—… íŒŒì¼ì—ì„œ)"""
    print("=" * 60)
    print(f"ë‹¨ì¼ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜: {table_name}")
    print("=" * 60)
    
    # ìŠ¤í‚¤ë§ˆ íŒŒì¼ í™•ì¸
    schema = load_table_schema(table_name)
    if not schema:
        print(f"âœ— ìŠ¤í‚¤ë§ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {table_name}")
        return False
    
    # ë°ì´í„° íŒŒì¼ í™•ì¸
    data = load_table_data(table_name)
    if data is None:
        print(f"âœ— ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {table_name}")
        return False
    
    print(f"âœ“ ìŠ¤í‚¤ë§ˆ íŒŒì¼ ë°œê²¬: {table_name}")
    print(f"âœ“ ë°ì´í„° íŒŒì¼ ë°œê²¬: {len(data)}ê°œ í–‰")
    
    # Supabase ì—°ê²°
    password = load_supabase_password()
    if not password:
        print("âœ— Supabase ë¹„ë°€ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    keys_file = os.path.join(os.path.dirname(__file__), 'supabase_keys.json')
    connection_string = None
    if os.path.exists(keys_file):
        with open(keys_file, 'r', encoding='utf-8') as f:
            keys = json.load(f)
            connection_string = keys.get('connection_string')
    
    try:
        if connection_string:
            parsed = parse_connection_string(connection_string)
            conn_params = {
                'host': parsed['host'],
                'port': parsed['port'],
                'database': parsed['database'],
                'user': parsed['user'],
                'password': password,
                'sslmode': 'require',
                'connect_timeout': 10
            }
        else:
            conn_params = {
                'host': SUPABASE_CONFIG['host'],
                'port': SUPABASE_CONFIG['port'],
                'database': SUPABASE_CONFIG['database'],
                'user': SUPABASE_CONFIG['user'],
                'password': password,
                'sslmode': 'require',
                'connect_timeout': 10
            }
        
        print(f"\nSupabase ì—°ê²° ì¤‘...")
        conn = psycopg2.connect(**conn_params)
        conn.autocommit = False
        cursor = conn.cursor()
        print(f"âœ“ Supabase ì—°ê²° ì„±ê³µ!")
        
        # í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜
        if migrate_table(cursor, table_name):
            conn.commit()
            print(f"\nâœ“ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {table_name}")
            return True
        else:
            conn.rollback()
            print(f"\nâœ— ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {table_name}")
            return False
        
    except Exception as e:
        print(f"âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()
        print("Supabase ì—°ê²° ì¢…ë£Œ")


def migrate_tables_from_backup(table_names: List[str]):
    """ë°±ì—… íŒŒì¼ì—ì„œ íŠ¹ì • í…Œì´ë¸”ë“¤ë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
    print("=" * 60)
    print(f"ë°±ì—…ì—ì„œ í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜: {len(table_names)}ê°œ")
    print("=" * 60)
    
    success_count = 0
    fail_count = 0
    
    for table_name in table_names:
        print(f"\n{'='*40}")
        if migrate_single_table(table_name):
            success_count += 1
        else:
            fail_count += 1
    
    print(f"\n" + "=" * 60)
    print(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {fail_count}ê°œ")
    print("=" * 60)


def main():
    """í†µí•© ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ í™•ì¸
    if len(sys.argv) > 1:
        if sys.argv[1] == '--table':
            # íŠ¹ì • í…Œì´ë¸”ë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜
            if len(sys.argv) > 2:
                table_names = sys.argv[2:]
                migrate_tables_from_backup(table_names)
                return
            else:
                print("ì‚¬ìš©ë²•: python full_migration.py --table <í…Œì´ë¸”ëª…1> [í…Œì´ë¸”ëª…2] ...")
                return
        elif sys.argv[1] == '--help':
            print("ì‚¬ìš©ë²•:")
            print("  ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜: python full_migration.py")
            print("  íŠ¹ì • í…Œì´ë¸”ë§Œ: python full_migration.py --table <í…Œì´ë¸”ëª…1> [í…Œì´ë¸”ëª…2] ...")
            return
    
    print("=" * 60)
    print("MySQL/MariaDB â†’ Supabase í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜")
    print("=" * 60)
    
    # 1ë‹¨ê³„: MySQL ë°±ì—…
    tables = backup_from_mysql()
    
    if not tables:
        print("\nâœ— ë°±ì—… ì‹¤íŒ¨ë¡œ ì¸í•´ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    # 2ë‹¨ê³„: Supabase ë§ˆì´ê·¸ë ˆì´ì…˜
    success = migrate_to_supabase(tables)
    
    if success:
        print("\n" + "=" * 60)
        print("ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        print("=" * 60)
    else:
        print("\n" + "=" * 60)
        print("ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print("ë°±ì—… íŒŒì¼ì€ ì €ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("=" * 60)


if __name__ == '__main__':
    main()

